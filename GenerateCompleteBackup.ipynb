{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of file name with path to csv\n",
    "# ['./Raw-Backups/kenmei-export-2022-08-27T17_19_00Z.csv']\n",
    "def get_csv_file_path():\n",
    "  csv_file_list = sorted(os.listdir(path=\"./Raw-Backups/\"))\n",
    "  location = \"./Raw-Backups/{}\"\n",
    "  return [\n",
    "    location.format(file_name)\n",
    "    for file_name in csv_file_list\n",
    "    if file_name.endswith(\".csv\")\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recieve filename and return datetime of file creation\n",
    "def parse_datetime_from_filename(filename):\n",
    "  try:\n",
    "    # Extract datetime string from filename: 'kenmei-export-2022-08-27T17_19_00Z.csv'\n",
    "    dt_str = filename.split('kenmei-export-')[-1].replace('.csv', '').replace('_', ':').replace('T', ' ')\n",
    "    return datetime.datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%SZ\")\n",
    "  except Exception as e:\n",
    "    raise ValueError(f\"Could not parse datetime from filename '{filename}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read MasterBackup.xlsx file, if doesn't exist create MasterBackup.xlsx\n",
    "def load_history(filepath):\n",
    "  EXPECTED_COLUMNS = [\n",
    "    \"title\", \"status\", \"score\", \"last_volume_read\", \"last_chapter_read\",\n",
    "    \"last_chapter_title_read\", \"last_read_at\", \"migratable\",\n",
    "    \"source_to_be_removed_at\", \"notes\", \"tracked_site\", \"series_url\", \"tags\"\n",
    "  ]\n",
    "\n",
    "  if os.path.exists(filepath):\n",
    "    sheet_data = pd.read_excel(filepath, sheet_name=None)\n",
    "\n",
    "    # Pad MasterData if any columns are missing\n",
    "    if \"MasterData\" in sheet_data:\n",
    "      df = sheet_data[\"MasterData\"]\n",
    "      for col in EXPECTED_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "          df[col] = pd.NA\n",
    "      sheet_data[\"MasterData\"] = df[EXPECTED_COLUMNS]  # reorder columns\n",
    "    else:\n",
    "      sheet_data[\"MasterData\"] = pd.DataFrame(columns=EXPECTED_COLUMNS)\n",
    "\n",
    "    # Ensure ImportHistory at least has default structure\n",
    "    if \"ImportHistory\" not in sheet_data:\n",
    "      sheet_data[\"ImportHistory\"] = pd.DataFrame(columns=[\"filename\", \"file_datetime\", \"imported_at\"])\n",
    "\n",
    "    return sheet_data\n",
    "\n",
    "  else:\n",
    "    return {\n",
    "      \"MasterData\": pd.DataFrame(columns=EXPECTED_COLUMNS),\n",
    "      \"ImportHistory\": pd.DataFrame(columns=[\"filename\", \"file_datetime\", \"imported_at\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge source_to_be_removed_at and source_removed_at columns and delete source_removed_at column\n",
    "def merge_source_columns(df):\n",
    "  # If both columns exist, merge them\n",
    "  if \"source_removed_at\" in df.columns and \"source_to_be_removed_at\" in df.columns:\n",
    "    # Fill NaNs in source_to_be_removed_at with values from source_removed_at\n",
    "    df[\"source_to_be_removed_at\"] = df[\"source_to_be_removed_at\"].combine_first(df[\"source_removed_at\"])\n",
    "    # Drop the old column\n",
    "    df.drop(columns=[\"source_removed_at\"], inplace=True)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save MasterBackup.xlsx file\n",
    "def save_master(filepath, master_df, history_df):\n",
    "  with pd.ExcelWriter(filepath, engine='openpyxl', mode='w') as writer:\n",
    "    master_df.to_excel(writer, index=False, sheet_name=\"MasterData\")\n",
    "    history_df.to_excel(writer, index=False, sheet_name=\"ImportHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively Update MasterBackup content\n",
    "def recursive_merge(files, idx, master_df, history_df, master_path):\n",
    "  if idx >= len(files):\n",
    "    print(\"‚úÖ All files processed.\")\n",
    "    master_df = merge_source_columns(master_df)\n",
    "    save_master(master_path, master_df, history_df)\n",
    "    return\n",
    "\n",
    "  current_file = files[idx]\n",
    "  file_dt = parse_datetime_from_filename(current_file)\n",
    "  filename = os.path.basename(current_file)\n",
    "\n",
    "  # Skip if already imported\n",
    "  if filename in history_df[\"filename\"].values:\n",
    "    print(f\"‚ö†Ô∏è {filename} already imported. Skipping.\")\n",
    "    return recursive_merge(files, idx + 1, master_df, history_df, master_path)\n",
    "\n",
    "  print(f\"üìÇ Importing {filename}\")\n",
    "\n",
    "  new_df = pd.read_csv(current_file)\n",
    "\n",
    "  # 1. Ensure all new columns exist in master\n",
    "  for col in new_df.columns:\n",
    "    if col not in master_df.columns:\n",
    "      master_df[col] = None\n",
    "\n",
    "  # 2. Ensure all master columns exist in new_df (fill if missing)\n",
    "  for col in master_df.columns:\n",
    "    if col not in new_df.columns:\n",
    "      new_df[col] = None\n",
    "\n",
    "  # 3. Update records (assumes \"series_url\" is unique key)\n",
    "  for _, new_row in new_df.iterrows():\n",
    "    match = master_df[master_df['series_url'] == new_row['series_url']]\n",
    "    if not match.empty:\n",
    "      index = match.index[0]\n",
    "      for col in new_df.columns:\n",
    "        if pd.notna(new_row[col]) and master_df.at[index, col] != new_row[col]:\n",
    "          master_df.at[index, col] = new_row[col]\n",
    "    else:\n",
    "      master_df = pd.concat([master_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "  # 4. Record history\n",
    "  history_df = pd.concat([history_df, pd.DataFrame([{\n",
    "    \"filename\": filename,\n",
    "    \"file_datetime\": file_dt,\n",
    "    \"imported_at\": datetime.datetime.now()\n",
    "  }])], ignore_index=True)\n",
    "\n",
    "  return recursive_merge(files, idx + 1, master_df, history_df, master_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "master_backup_filename = \"MasterBackup.xlsx\"\n",
    "\n",
    "# Step 1: Get all CSVs and sort them by datetime extracted from filename\n",
    "all_csvs = get_csv_file_path()\n",
    "\n",
    "# Step 2: Load existing master and history\n",
    "loaded = load_history(master_backup_filename)\n",
    "master_df = loaded[\"MasterData\"]\n",
    "history_df = loaded[\"ImportHistory\"]\n",
    "\n",
    "# Step 3: Start recursive import\n",
    "recursive_merge(all_csvs, 0, master_df, history_df, master_backup_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
