{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of file name with path to csv\n",
    "def get_csv_file_path():\n",
    "  csv_file_list = sorted(os.listdir(path=\"./Raw-Backups/\"))\n",
    "  location = \"./Raw-Backups/{}\"\n",
    "  return [\n",
    "    location.format(file_name)\n",
    "    for file_name in csv_file_list\n",
    "    if file_name.endswith(\".csv\")\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recieve filename and return datetime of file creation\n",
    "def parse_datetime_from_filename(filename):\n",
    "  try:\n",
    "    # Extract datetime string from filename: 'kenmei-export-2022-08-27T17_19_00Z.csv'\n",
    "    dt_str = filename.split('kenmei-export-')[-1].replace('.csv', '').replace('_', ':').replace('T', ' ')\n",
    "    return datetime.datetime.strptime(dt_str, \"%Y-%m-%d %H:%M:%SZ\")\n",
    "  except Exception as e:\n",
    "    raise ValueError(f\"Could not parse datetime from filename '{filename}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index series_url column to use as PK\n",
    "def enforce_primary_key(df, key_column=\"\"):\n",
    "  if df[key_column].isnull().any():\n",
    "    raise ValueError(f\"Primary key column '{key_column}' contains null values.\")\n",
    "\n",
    "  if not df[key_column].is_unique:\n",
    "    raise ValueError(f\"Primary key constraint violated: duplicates found in '{key_column}'.\")\n",
    "\n",
    "  return df.set_index(key_column, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update merge backup file with data from new file and override rows where PK already exists\n",
    "def upsert_on_primary_key(master_df, new_df, pk=\"series_url\"):\n",
    "    # Set PK as index and validate uniqueness\n",
    "    master_df = enforce_primary_key(master_df, key_column=pk)\n",
    "    new_df = enforce_primary_key(new_df, key_column=pk)\n",
    "\n",
    "    # Drop overlapping rows from master_df\n",
    "    overlapping_keys = master_df.index.intersection(new_df.index)\n",
    "    if not overlapping_keys.empty:\n",
    "        master_df = master_df.drop(index=overlapping_keys)\n",
    "\n",
    "    # Combine both ‚Äî new rows overwrite old ones\n",
    "    combined_df = pd.concat([master_df, new_df])\n",
    "\n",
    "    # Optional: sort by key\n",
    "    return combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read merged backup file, if merged backup file doesn't exist create it\n",
    "def load_history(filepath):\n",
    "  DATA_COLUMNS = [\n",
    "    \"title\", \"status\", \"last_volume_read\", \"last_chapter_read\",\n",
    "    \"last_chapter_title_read\", \"last_read_at\", \"tracked_site\", \"series_url\",\n",
    "    \"migratable\", \"source_to_be_removed_at\", \"notes\", \"tags\", \"score\"\n",
    "  ]\n",
    "\n",
    "  HISTORY_COLUMNS = [\"filename\", \"file_datetime\", \"imported_at\"]\n",
    "\n",
    "  if os.path.exists(filepath):\n",
    "    sheet_data = pd.read_excel(filepath, sheet_name=None)\n",
    "\n",
    "    # Pad Data if any columns are missing\n",
    "    if \"Data\" in sheet_data:\n",
    "      df = sheet_data[\"Data\"]\n",
    "      for col in DATA_COLUMNS:\n",
    "        if col not in df.columns:\n",
    "          df[col] = pd.NA\n",
    "      sheet_data[\"Data\"] = df[DATA_COLUMNS]  # reorder columns\n",
    "    else:\n",
    "      sheet_data[\"Data\"] = pd.DataFrame(columns=DATA_COLUMNS)\n",
    "\n",
    "    # Ensure ImportHistory at least has default structure\n",
    "    if \"ImportHistory\" not in sheet_data:\n",
    "      sheet_data[\"ImportHistory\"] = pd.DataFrame(columns=HISTORY_COLUMNS)\n",
    "\n",
    "    return sheet_data\n",
    "\n",
    "  else:\n",
    "    return {\n",
    "      \"Data\": pd.DataFrame(columns=DATA_COLUMNS),\n",
    "      \"ImportHistory\": pd.DataFrame(columns=HISTORY_COLUMNS)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge source_to_be_removed_at and source_removed_at columns and delete source_removed_at column\n",
    "def merge_source_columns(df):\n",
    "  # If both columns exist, merge them\n",
    "  if \"source_removed_at\" in df.columns and \"source_to_be_removed_at\" in df.columns:\n",
    "    # Fill NaNs in source_to_be_removed_at with values from source_removed_at\n",
    "    df[\"source_to_be_removed_at\"] = df[\"source_to_be_removed_at\"].combine_first(df[\"source_removed_at\"])\n",
    "    # Drop the old column\n",
    "    df.drop(columns=[\"source_removed_at\"], inplace=True)\n",
    "\n",
    "  return df.sort_values(\"title\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged backup file file\n",
    "def save_master(filepath, master_df, history_df):\n",
    "  with pd.ExcelWriter(filepath, engine='openpyxl', mode='w') as writer:\n",
    "    master_df.to_excel(writer, index=False, sheet_name=\"Data\")\n",
    "    history_df.to_excel(writer, index=False, sheet_name=\"ImportHistory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively update merged backup file\n",
    "def recursive_merge(files, idx, master_df, history_df, master_path):\n",
    "  if idx >= len(files):\n",
    "    print(\"‚úÖ All files processed.\")\n",
    "    master_df = merge_source_columns(master_df)\n",
    "    save_master(master_path, master_df, history_df)\n",
    "    return\n",
    "\n",
    "  current_file = files[idx]\n",
    "  file_dt = parse_datetime_from_filename(current_file)\n",
    "  filename = os.path.basename(current_file)\n",
    "\n",
    "  # Skip if already imported\n",
    "  if filename in history_df[\"filename\"].values:\n",
    "    print(f\"‚ö†Ô∏è {filename} already imported. Skipping.\")\n",
    "    return recursive_merge(files, idx + 1, master_df, history_df, master_path)\n",
    "\n",
    "  print(f\"üìÇ Importing {filename}\")\n",
    "  new_df = pd.read_csv(current_file)\n",
    "\n",
    "  # Ensure both DataFrames have same columns\n",
    "  all_columns = set(master_df.columns).union(set(new_df.columns))\n",
    "\n",
    "  for col in all_columns:\n",
    "    if col not in master_df.columns:\n",
    "      master_df[col] = None\n",
    "    if col not in new_df.columns:\n",
    "      new_df[col] = None\n",
    "\n",
    "  # Replace manual loop with upsert\n",
    "  master_df = upsert_on_primary_key(master_df, new_df, pk=\"series_url\")\n",
    "\n",
    "  # Record import in history\n",
    "  history_df = pd.concat([history_df, pd.DataFrame([{\n",
    "    \"filename\": filename,\n",
    "    \"file_datetime\": file_dt,\n",
    "    \"imported_at\": datetime.datetime.now()\n",
    "  }])], ignore_index=True)\n",
    "\n",
    "  return recursive_merge(files, idx + 1, master_df, history_df, master_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define merged backup file\n",
    "backup_filename = \"MasterBackup\"\n",
    "backup_file = f\"{backup_filename}.xlsx\"\n",
    "\n",
    "# Step 1: Get all CSVs and sort them by datetime extracted from filename\n",
    "all_csvs = get_csv_file_path()\n",
    "\n",
    "# Step 2: Load existing master and history\n",
    "loaded = load_history(backup_file)\n",
    "master_df = loaded[\"Data\"]\n",
    "history_df = loaded[\"ImportHistory\"]\n",
    "\n",
    "# Step 3: Start recursive import\n",
    "recursive_merge(all_csvs, 0, master_df, history_df, backup_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
